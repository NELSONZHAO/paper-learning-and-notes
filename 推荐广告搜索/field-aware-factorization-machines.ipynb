{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 摘要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二阶叉乘特征的FM模型在计算广告ctr预估领域得到广泛应用。最近我们提出了FM的变体，FFM，在一些全球CTR预估比赛中取得了超过FM的成绩，因此在本文中我们提出了一个有效应用在大规模稀疏数据下的FFM模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CTR预估在广告领域扮演着重要角色，逻辑回归是主要使用的模型。给定m个训练样本$(y_i, x_i)$，模型参数向量$w$通过优化以下函数得到：\n",
    "\n",
    "$$\\min_w \\frac{\\lambda}{2}||w||^2_2 + \\sum_{i=1}^m\\log(1+\\exp(-y_i\\phi_{LM}(w, x_i)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$\\phi_{LM}(w, x)=w\\cdot x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 上述损失函数是等价于logloss的：\n",
    "\n",
    "> 当$y_i=1$时：\n",
    "> \n",
    "> $$\\log(1+\\exp(-w\\cdot x))=-\\log(\\frac{1}{1+\\exp(-w\\cdot  x)})=-\\log \\hat{y}$$\n",
    "> \n",
    "> 当$y_i=-1$时：\n",
    "> \n",
    "> $$\\log(1+\\exp(w\\cdot x))=-\\log(\\frac{1}{1+\\exp(w\\cdot  x)})=-\\log (1-\\hat{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在广告CTR预估中，让模型学习联合特征（feature conjunctions）是非常关键的。以下图为例：\n",
    "\n",
    "![](pictures/ffm-example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以发现Gucci在Vogue上的点击率远高于在其它Publisher上的点击率，但是对于线性模型来说，Gucci \\times Vogue具有高点击率这一信息是难以学习到的，因为对于线性模型来说，Gucci和Vogue的权重是分别学习的（Learning Separately）。\n",
    "\n",
    "为了解决这个问题，可以使用两种不同的方式：\n",
    "- 模型1：degree-2多项式模型，可以对所有特征进行两两组合，学习到所有的feature conjunctions\n",
    "- 模型2：FM模型，学习特征隐向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 作者提到，FM的一个变体叫Pairwise Interactiion Tensor Factorization(PITF)，在2012KDD Cup上，PITF的通用形式”factor model“被提出，为了避免和PITF模型混淆，作者将本文的模型命名为”Field-aware Factorization Machine“。PITF和FFM的区别在于，PITF仅考虑了三个fields：user, item, tag（PITF最开始被提出是用在推荐领域的），FFM则是更通用的一个模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要贡献：\n",
    "- 进一步证明FFM的有效性\n",
    "- 将FFM和Poly2、FM进行比较，从概念上和实验上进行比较和讨论\n",
    "- 展示一种并行化的训练FFM的方式，并使用early-stopping来避免过拟合\n",
    "- 开源FFM的package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Poly2 And FM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poly2模型：\n",
    "\n",
    "$$\\phi_{Poly2}(w,x)=\\sum_{j_1=1}^n \\sum_{j_2=j_1+1}^n w_{h(j_1,j_2)}x_{j_1} x_{j_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$h(j_1, j_2)$是将$j_1$和$j_2$映射到natural number的函数，对于Poly2，其计算的时间复杂度为$O(\\bar{n}^2)$（$\\bar{n}$是平均每个样本的非零元素个数）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FM模型：\n",
    "\n",
    "$$\\phi_{FM}(w, x)=\\sum_{j_1=1}^n\\sum_{j_2=j_1+1}^n \\langle w_{j_1}\\cdot w_{j_2}\\rangle x_{j_1} x_{j_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FM通过将特征交乘项前的参数进行因式分解，学习到每个特征对应的$k$维隐向量，时间复杂度为$O(\\bar{n}k$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poly2和FM相比有以下缺点：\n",
    "- Poly2本身是对训练数据中的交乘特征进行memorization，相比于FM的因式分解，泛化性能更差\n",
    "- Poly2对训练样本中没有出现的特征交叉无法进行参数学习\n",
    "- Poly2的计算时间复杂度相比于FM更高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 作者提到特征交叉的实现使用了Hash函数：$h(j_1, j_2)=(\\frac{1}{2}(j_1+j_2)(j_1+j_2+1)+j_2) \\mod B$，其中B是用户指定的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 FFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFM在FM基础上增加了Field的概念，例如在上面的例子中，ESPN、Vogue、NBC都属于Publisher这个field，Nike，Adidas，Cucci都属于Advertiser这个field。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/ffm-case.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑上面这个样本，在FM中，每个feature有且仅有一个latent vector。以ESPN为例，$v_{ESPN}$既被用来学习$v_{ESPN}\\cdot v_{NIKE}$，也被用来学习$v_{ESPN}\\cdot v_{Male}$。但由于$NIKE$这个feature和$Male$这个feature是属于不同的field的，因此实际中$v_ESPN$对于这两个field的影响是不一样的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在FFM中引入Field的概念之后，就有$\\phi_{FFM}(w, x)=v_{ESPN, A}\\cdot v_{NIKE, P} + v_{ESPN, G} \\cdot v_{Male, P} + v_{NIKE, G}\\cdot v_{Male, A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，对于不同field的特征，$ESPN$的隐藏向量是不同的。例如对于和field=Advertiser, feature=NIKE来说，$v_{ESPN}$使用了针对Advertiser的隐藏向量$v_{ESPN, A}$。更一般化的，FFM的数学表达为（忽略线性项和截距项）：\n",
    "\n",
    "$$\\phi_{FFM}(w, x)=\\sum_{j_1=1}^n \\sum_{j_2=j_1+1}^n \\langle v_{j_1, f_2}\\cdot v_{j_2, f_1}\\rangle x_{j_1} x_{j_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$f_1$和$f_2$分别对应$j_1$和$j_2$的field。FFM中参数个数为$nfk$个，计算时间复杂度为$O(\\bar{n}^2k)$。\n",
    "\n",
    "在FFM中，由于每个latent vector只需要学习对于某个field的影响，因此实际中$k_{FFM} << k_{FM}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各个模型的时间复杂度比较：\n",
    "\n",
    "![](pictures/model-compare.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 FFM的求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFM的优化采用AdaGrad进行参数优化（AdaGrad对于matrix factorization的求解非常高效）。\n",
    "\n",
    "在每一轮中随机sample一个样本来优化$v_{j_1, f_2}$和$v_{j_2, f_1}$，由于在广告场景下，我们的样本特征向量$x$是非常稀疏的，我们只需要更新非零位置对应的参数即可（例如word2vec的skip-gram模型中，每一轮的词向量只更新center word）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/ffm-training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在初始化中，权重符合$[0, \\frac{1}{\\sqrt{k}}]$区间的均匀分布。\n",
    "\n",
    "作者提到将每个样本归一化到单位模（have the unit length）会使得test上准确率提高，并且对于参数的变化更加鲁棒。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Parallelization on Shared-memory Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练过程使用多线程并行化异步方式，加速训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Adding Field Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在机器学习中常用的数据结构有LIBSVM：label feat1:val1 feat2:val2 ...（XGBoost使用的就是这种结构）。\n",
    "\n",
    "对于FFM，作者基于LIBSVM对数据结构进行了扩展：label field1:feat1:val1 field2:feat2:val2 ...。相当于对于每个field下的每个feature，要给定一个val值，作者讨论了Categorical，Numerical以及Single-field三种特征的构造方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical特征一般直接做One Hot Encoding，存储为LIBSVM格式时，仅存储非零位即可。\n",
    "\n",
    "例如：样本**Yes P:ESPN A:NIKE G:Male**\n",
    "\n",
    "转为FFM LIBSVM格式为：**Yes P:P-ESPN:1 A:A-NIKE:1 G:G-Male:1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在推荐广告中，用户的年龄、广告的历史cost、show都是连续值实数特征。作者在文中以论文是否被接收为例，对于连续值特征的FFM LIBSVM表示提出了两种解决方案：\n",
    "\n",
    "![](pictures/ffm-libsvm-numerical.png)\n",
    "\n",
    "第一种：**Yes AR:AR:45.73 Hidx:Hidx:2 Cite:Cite:3**，即把每个numerical feature都作为一个field。优点是简单，缺点是这种field仅仅是对feature的复制，并没有提供额外的信息。\n",
    "\n",
    "第二种：**Yes AR:45:1 Hidx:2:1 Cite:3:1**，即把原始numerical feature离散化之后再进行转换。缺点是离散化方式难以确定，同时离散化过程会损失一部分信息。\n",
    "\n",
    "> 个人更偏向于第二种，一方面是因为离散化可以让模型的线性部分学到非线性特征，另一方面是方便特征的统一管理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Single-field Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single-field feature是指例如NLP数据中的句子，当然在广告中也可以是广告标题、广告内容文本等：\n",
    "\n",
    "![](pictures/ffm-libsvm-single-field.png)\n",
    "\n",
    "当我们把所有words归结为一个field，此时FFM退化为FM。有读者可能会问为什么不像Numerical feature中那样构造dummy field？作者提出FFM的model size是$O(nfk)$，当构造dummy field时，$f=n$，而$n$又很大，是不现实的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
