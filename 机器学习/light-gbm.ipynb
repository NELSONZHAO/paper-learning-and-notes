{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 摘要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBDT集成树算法在机器学习中应用十分广泛，但在实际工程中，随着样本规模与特征规模的增大，算法性能和可扩展性遇到了瓶颈，主要原因在于**算法需要扫描所有的样本去估计所有可能分裂点的信息增益**，这一步是十分消耗时间的。\n",
    "\n",
    "本文提出了两种创新技术分别从**样本数量**与**特征数量**上来解决这个问题：\n",
    "\n",
    "- **Gradient-based One-side Sampling（GOSS）**：通过对小梯度样本的采样，减少计算信息增益的数据量。由于具有较大梯度样本在信息增益计算中较重要，因此采用GOSS能够在减少样本量的情况下获得准确的估计；\n",
    "- **Exclusive Feature Bundling（EFB)**：通过绑定互斥特征（例如OHE）来减少特征数量。寻找最优绑定特征是一个NP-Hard问题，因此采用了贪心算法来获得近似解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "传统GBDT实现会去扫描所有样本计算所有候选分裂点的信息增益，这样分裂时的时间复杂度与样本数量和特征数量成正比。因此，最直接的想法就是去减少样本与特征数量。尽管现在有一些算法通过样本的权重对数据进行采样，但由于GBDT算法本身不支持带权样本，因此这种方式无法直接应用到GBDT上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Gradient-based One-side Sampling\n",
    "\n",
    "在GBDT算法中，样本本身是无权重的。但作者发现样本不同梯度会影响信息增益的计算。具体来说，**梯度绝对值大的样本对计算信息增益的贡献更大**。因此当我们需要对样本采样的时候，首先应该保留这些梯度绝对值大的样本，而采样那些梯度绝对值较小的样本。作者证明了这种方法比随机样本采样性能更好。\n",
    "\n",
    "> 关于梯度绝对值越大，对于信息增益计算贡献越大的理解。\n",
    "> - 角度1：对一个凸函数来说，越靠近（局部）极小值的地方，函数的斜率（梯度）越接近于0，离函数的极小值越远，梯度越大。因此梯度本身可以反映当前距离最优解的远近；\n",
    "> - 角度2：对于GBDT来说，采用mse损失函数时，样本在t轮的负梯度就是前t-1预测值和真实值的残差。当梯度（绝对值）越大时，意味着当前预测值和真实值之间的差距较大，当前样本没有拟合好；当梯度（绝对值）越小时，意味着当前预测值和真实值之间的差距越小，当前样本已经拟合地比较好，可以被丢掉；\n",
    "> - 角度3：从信息增益角度来说，在CART中，一个子树的信息增益采用分裂后的左右叶子结点的平方误差和不分裂之前父节点的平方误差计算。因此当在计算分裂点的信息增益时，梯度（绝对值）较大的样本意味着残差也越大，因而对信息增益计算贡献就很大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Exclusive Feature Bundling\n",
    "\n",
    "在实际应用中，我们的特征往往是高维稀疏的，这就意味着有可能设计某种算法几乎无损的算法对这些特征进行降维。\n",
    "\n",
    "在稀疏特征空间内，许多特征取值几乎都是互斥的，例如最极端的状况就是One-hot Encoding，做了One-hot Encoding后，这几列不会同时在一个样本上取非零值。类似这类特征，作者提出了通过绑定的方式减少维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 基础知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "集成树模型最耗时的阶段在于计算信息增益，其主要包括以下步骤：\n",
    "- 1.排序特征提出候选分裂点\n",
    "- 2.遍历所有样本计算每个候选分裂点的信息增益\n",
    "- 3.从中选出信息增益最大的特征及对应分裂点\n",
    "\n",
    "目前比较常用的寻找分裂点的方式主要包括两种：\n",
    "\n",
    "- 1.Pre-sort Algortithm：基于预排序好的特征值寻找最优分裂点\n",
    "- 2.Histogram-based Algorithm：将连续特征值离散化为不同的bins，利用这些bins去构建特征直方图\n",
    "\n",
    "考虑到histogram-based算法时间和空间复杂度上都更加高效，作者后续的创新都是基于这种分裂点搜索方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**时间复杂度计算**：\n",
    "\n",
    "- 在histogram构建阶段，需要扫描所有样本和所有特征，因此时间复杂度是$O(\\#data\\times \\#features)$\n",
    "- 在分裂点寻找阶段，每个特征都已经被分成了不同的bin，因此时间复杂度是$O(\\#bin \\times \\#features)$\n",
    "\n",
    "因此作者的思路就是去减少data和features，从而优化整个计算时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 GOSS算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 算法描述\n",
    "\n",
    "GBDT的样本梯度可以作为样本的权重信息，但同时要考虑采样后的样本分布，因此GOSS分为两个步骤：\n",
    "\n",
    "- 1.按照当前样本的梯度绝对值降序排列，选择前$a\\times 100\\%$个大梯度样本；\n",
    "- 2.随机从剩余的样本中随机采样$b\\times 100\\%$个小梯度样本；\n",
    "- 3.对于小梯度样本，采样后分布发生变化，此时GOSS在计算信息增益时，对小梯度样本都乘以$\\frac{1-a}{b}$维持原有样本分布。\n",
    "\n",
    "![](pictures/lightgbm-goss.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram-based Algorithm分析：\n",
    "\n",
    "- 最外层遍历树的层数；\n",
    "- 第二层遍历当前level的结点集合。对于每个结点，获取需要使用的样本集合；\n",
    "- 遍历k=1到m每个特征，开始构建直方图；可以采用最简单的分位点作为切分bin的阈值；\n",
    "- 遍历样本集合中每个样本，将其第k个特征的值映射到相应的bin中，并更新直方图bin的统计量：累积y的值以及累积样本个数\n",
    "- 当构建完当前第k个特征的直方图后，便可以选择第k个特征的最佳分裂点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 理论分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GDBT使用决策树去学习一个从当前输入空间$\\mathcal{X}^s$到梯度空间$\\mathcal{G}$的函数。\n",
    "\n",
    "假设我们有由n个i.i.d的训练样本组成的数据集合$\\{x_1,\\cdots,x_n\\}$，其中$x_i$是一个$s$维的向量，$x_i\\in \\mathcal{X}^s$。定义在每一轮迭代中，损失函数关于模型输出的负梯度为$\\{g_1,\\cdots,g_n\\}$。对于GBDT，通常使用分裂前后的方差计算信息增益。\n",
    "\n",
    "定义$O$为在决策树中一个固定结点上的训练样本集合。按照特征$j$的值$d$分裂该结点，其相对增益量为：\n",
    "\n",
    "$$V_{j|O}(d)=\\frac{1}{n_O}(\\frac{(\\sum_{\\{x_i\\in O_{:x_{ij}\\le d}\\}}g_i)^2}{n_{l|O}^j(d)} + \\frac{(\\sum_{\\{x_i\\in O_{:x_{ij}\\gt d}\\}}g_i)^2}{n_{r|O}^j(d)})$$\n",
    "\n",
    "其中$n_O=\\sum I[x_i\\in O]$，$n_{l|O}^j=\\sum I[x_i\\in O_{:x_{ij}\\le d}]$，$n_{r|O}^j=\\sum I[x_i\\in O_{:x_{ij}\\gt d}]$\n",
    "\n",
    "> 通俗来讲，上面的公式就是，按照特征$j$的$d$阈值将样本分为左右两部分，使得左子树中的样本均方误差加右子树中的样本均方误差最大。\n",
    "> 类比XGBoost的信息增益$\\mathcal{L}_{split}=\\mathcal{L}_I-(\\mathcal{L}_{I_L}+\\mathcal{L}_{I_R})=\\frac{1}{2}\\sum_{j=1}^T[\\frac{(\\sum_{i\\in I_L}g_i)^2}{\\sum_{i\\in I_L}h_i+\\lambda} + \\frac{(\\sum_{i\\in I_R}g_i)^2}{\\sum_{i\\in I_R}h_i+\\lambda} - \\frac{(\\sum_{i\\in I}g_i)^2}{\\sum_{i\\in I}h_i+\\lambda}]-\\gamma$。对于GBDT来说，未使用二阶梯度，因此分母都是常量，即可以退化为上述lightGBM中的增益计算公式。\n",
    "\n",
    "对于特征$j$，决策树选择最优点$d_j^*=\\arg \\max_d V_j(d)$计算最大的信息增益$V_j(d_j^*)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GOSS**\n",
    "\n",
    "在GOSS中，首先按照梯度绝对值降序排序，选出前$a\\times 100\\%$个样本构成集合$A$；在剩余样本集合$A_c$中选取$b\\times 100\\%$个样本得到集合$B$。\n",
    "\n",
    "因此，我们采取GOSS计算样本集合$A\\cup B$的信息增益为：\n",
    "\n",
    "$$\\tilde{V}_{j}(d)=\\frac{1}{n}(\\frac{(\\sum_{x_i\\in A_l}g_i + \\frac{1-a}{b}\\sum_{x_i\\in B_l} g_i)^2}{n_{l}^j(d)} + \\frac{(\\sum_{\\{x_i\\in A_r}g_i + \\frac{1-a}{b}\\sum_{x_i\\in B_r} g_i)^2}{n_{r}^j(d)})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用上面这种方式，我们可以得到信息增益的近似值，损失的精度非常小；同时极大地降低了计算时间复杂度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 EFB算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作者提到高维稀疏特征中，特征与特征之间几乎不会同时在一个样本上取非零元素，其中最极端的就是OHE变量，对于同一个样本，做了OHE后不会同时取非零（只有一个位置取非零）。因此，可以设计一种近似无损的合并方法。合并后，我们在构建特征直方图时，时间复杂度就会从$O(\\# data\\times \\# features)$降到$O(\\# data \\times \\# bundle)$。\n",
    "\n",
    "EFB算法分为两步：\n",
    "- 1.选择合并特征。判断哪些特征可以用来绑定；\n",
    "- 2.构造绑定特征。对给定需要绑定的特征进行绑定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 选择合并特征\n",
    "\n",
    "考虑到EFB的是一个NP-Hard问题，无法在多项式时间内解决。因此作者类比了“地图填色“问题，将特征比作结点，特征和特征之间如果不互斥就连一条边，进而使用贪心算法来解决。\n",
    "\n",
    "除此之外，很多特征虽然不是完全互斥的，但是是近似互斥，如果能够容忍部分冲突率，可以进一步降低特征维度。作者提到，随机干扰一小部分特征值，会影响模型准确率最大为$\\mathcal{O}([(1-\\gamma)n]^{-2/3})$，其中$\\gamma$是一个bundle中的最大冲突率。只要我们选的$\\gamma$比较小，我们就可以在效率和准确率之间得到一个好的平衡。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**提出候选绑定特征算法：**\n",
    "\n",
    "- 1.构造一个无向无权图$G=(V,E)$，其中每个特征作为一个结点，特征与特征之间如果不互斥就连一条边，其中边的权重为两个特征之间的总冲突（两个特征的冲突数定义为在两个特征上同时取非零的样本数）；\n",
    "- 2.将特征按照度（degree）降序排列，即度最大的特征位于前部；\n",
    "- 3.遍历每个特征，并根据最小冲突$\\gamma$来判断是否绑定至现有bundle中或新创建一个bundle。\n",
    "\n",
    "![](pictures/lightgbm-efb-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 上述算法的时间复杂度为$O(\\# features ^2)$，主要消耗在于构造Graph的过程。当特征数量增大时，这个时间复杂度可能有些过大；因此作者提出了进一步的优化方案，即通过$O(\\# features)$时间复杂度算法计算每个特征值取非零的样本个数，接着按照非零个数取值排序即可（后面的算法不变）。这中方法实际是近似构建图方法的，在构建图时，特征排序方案是按照其degree来降序，当一个特征的非零取值数越多，意味着该特征越容易与其它特征发生冲突。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 特征绑定方法\n",
    "\n",
    "通过第一步的贪心算法确定了哪些特征需要绑定，进而需要结合一种算法将这些特征进行绑定，绑定算法的关键要满足**能够保证原特征在绑定特征中的可识别性**。\n",
    "\n",
    "考虑到histogram-based方法将原始特征离散化成多个bins，因此可以将互斥特征放置在不同的bins中来组成绑定特征。最简单的方式就是在原始特征上加阈值（offset）。例如，假设要绑定特征A和特征B，其中A的取值范围为$[0,10)$，B的取值范围为$[0, 20)$，此时我们可以在特征B上统一加$offset=10$将值域映射到$[10, 30)$。经过这样的操作，我们就可以将A和B绑定为新的特征，其值域为$[0,30)$。\n",
    "\n",
    "![](pictures/lightgbm-efb-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM主要从样本量和特征维度两个方面做了优化：\n",
    "\n",
    "- 通过GOSS来做样本采样，减少计算信息增益时的样本量\n",
    "- 通过EFB对稀疏特征降维，减少构建特征直方图时的计算量"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
