{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 多项式曲线拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假定有N个$x$的观察值，记做$\\mathbb{x}=(x_1,\\cdots,x_N)^T$，同时有各个$x$值对应的观察值$t$，记做$\\mathbb{t}=(t_1,\\cdots,t_N)^T$。\n",
    "\n",
    "假设$x$由$[0,1]$之间的均匀分布产生，$t$由$\\sin(2\\pi x)$计算后添加高斯噪声后得到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x10f587b00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ng0GS1GIwSJJa/j86w7g3nn/SfwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f47d668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 定义种子\n",
    "np.random.seed(777)\n",
    "# 定义点的个数\n",
    "N = 10\n",
    "# 生成x\n",
    "x = np.random.uniform(low=0.0, high=1.0, size=N)\n",
    "# 生成t\n",
    "t = np.array([np.sin(2*np.pi*point) + \n",
    "     np.random.normal(loc=0.0, scale=0.01) for point in x])\n",
    "# 画图\n",
    "plt.scatter(x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的目标是利用有限的训练数据集（N=10）样本来对（未来无限的）新的$\\hat{x}$进行预测，得到$\\hat{t}$。在上面的例子中就是去拟合$\\sin(2\\pi x)$曲线。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 函数假设"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以最简单的多项式假设来进行上述点的拟合，我们假设：\n",
    "\n",
    "$$y(x, \\mathbb{w})=w_0+w_1 x + w_2 x^2 +\\cdots + w_M x^M=\\sum_{j=0}^M w_j x^j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述式子虽然是关于$x$的非线性函数，但却是关于$\\mathbb{w}$的线性函数。**关于未知参数的线性函数，例如多项式函数，被称为Linear Model**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义好学习函数后，需要最小化error function来衡量学习结果$f(x,\\mathbb{w})$和训练数据集中真实结果的差异。以最简单的平方误差函数为例：\n",
    "\n",
    "$$E(\\mathbb{w})=\\frac{1}{2}\\sum_{n=1}^N \\{f(x_n,\\mathbb{w})-t_n\\}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过$\\min_w E(\\mathbb{w})$求得最优解$\\mathbb{w}^*$，得到最终的函数$y(x, \\mathbb{w}^*)$。\n",
    "\n",
    "由于这里的平方误差函数是关于参数$\\mathbb{w}$的二次函数，因此关于参数$\\mathbb{w}$的导数是线性的，因此最小化error function求得的$\\mathbb{w}$有闭式解和唯一解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设给定样本矩阵为$X\\in \\mathbb{R}^{N\\times F}$，其中每一行是一个样本，每一列是一个特征。目标$T\\in \\mathbb{R}^{N}$。则error function可以写作："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E(\\mathbb{w})=\\frac{1}{2}(X\\mathbb{w}-T)^T (X\\mathbb{w}-T)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当$X$可逆时，求解得:\n",
    "\n",
    "$$\\mathbb{w}=(X^T X)^{-1} X^TT$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 超参数选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多项式函数中我们可以选择阶数$M$的值，例如$M=0,1,2,3,\\cdots, 9$。如下图所示分别是$M=0,1,3,9$时模型拟合的结果："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ch1/order-of-polynomial.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 当$M=0$或$M=1$时，拟合结果是一条直线，拟合结果较差；\n",
    "- 当$M=3$时，拟合结果看起来很好；\n",
    "- 当$M=9$时，拟合的函数穿过了每一个训练数据集的点，此时$E(\\mathbb{w}^*)=0$。但是函数曲线震荡严重不够平滑，且对于$\\sin(2\\pi x)$函数的表示很差，这种情况被称为over-fitting。\n",
    "\n",
    "我们学习的目标是为了对于新的数据取得较好的预测结果，即习得泛化能力较强的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过计算RMSE(Root Mean Square Error)可以对不同阶数的拟合结果进行量化分析。定义：\n",
    "\n",
    "$$E_{RMS}=\\sqrt{2E(\\mathbb{w}^*)/N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ch1/rmse-error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到$M=9$时，函数在训练样本上的误差函数为0，但对于新的样本点误差非常大。\n",
    "\n",
    "9阶多项式相比于3阶多项式看似能够包含更多的信息，除此之外，$\\sin(2\\pi x)$的展开项包含了多有的阶，因此随着$M$的增加，我们的结果应该变得越来越好。**但实际上并非如此，除了阶以外，数据集本身过小，使得无法包含足够的pattern信息，反而阶越高越使得模型记住小数据量的异常信息**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 不同假设函数下的参数结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过实验可以发现，不同假设函数得到的参数结果如下。我们可以观察到，随着阶数$M$的增加，学习参数的量纲也逐渐变大；尤其对于$M=9$时，参数都是超大的整数或者负数，因此使得能够对训练样本中每个样本进行很好的拟合。\n",
    "\n",
    "![](ch1/coefficient.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 不同数据集下的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过将数据集由$N=15$扩展到$N=100$后，拟合的效果更接近于$\\sin(2\\pi x)$，过拟合的情况也得到缓解。\n",
    "\n",
    "> 有学者提出：一个启发式的选择超参数的方法是数据集的规模大小至少要是待学习参数的5倍或10倍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ch1/different-data-size.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何根据数据情况来找到合适的模型复杂度：在实际操作中往往通过正则化（Regularization）来控制过拟合状况，即通过在error function中添加惩罚项来防止学习到的参数量纲过大。\n",
    "\n",
    "$$\\tilde{E}(w)=\\frac{1}{2}\\sum_{n=1}^N\\{y(x_n, \\mathbb{w}) - t_n)^2\\} + \\frac{\\lambda}{2}||\\mathbb{w}||^2$$\n",
    "\n",
    "其中，$||\\mathbb{w}||^2=w_0^2+w_1^2+\\cdots+w_M^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**在正则化项中，一般不包含$w_0$，主要是排除结果中对于初始目标变量选择（截距项）的影响**。加入正则化项的error function也具有闭式解，被称为shrinkage methods或weight decay。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不同正则化超参数$\\lambda选择对于参数学习的影响："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ch1/regularization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 当$\\lambda$很小时，相当于没有加入惩罚项，其效果等同于未加入Regularization的回归\n",
    "- 当$\\lambda$适中时，过拟合问题得到缓解\n",
    "- 当$\\lambda$较大时，所有参数都非常接近于0，参数量纲被压缩。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Curve Fitting Revisited - 极大似然角度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 确定参数概率分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在曲线拟合问题上，我们的目的是习得一个函数，能够对新的输入点$x$进行预测得到目标值$t$。在具体操作上，可以通过**概率分布**来表示预测值$t$的不确定性。\n",
    "\n",
    "我们假设给定输入值$x$，对应的$t$值符合均值为$y(x,\\mathbb{w})$的高斯分布，则：\n",
    "\n",
    "$$p(t|x,\\mathbb{w},\\beta) = \\mathcal{N}(t|y(x, \\mathbb{w}), \\beta^{-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来就需要利用训练数据来学习位置参数$\\mathbb{w}$和$\\beta$。如果训练样本是从上述分布中独立采样的，则似然函数为：\n",
    "\n",
    "$$p(\\mathbb{t}|\\mathbb{x},\\mathbb{w},\\beta) = \\prod_{n=1}^N \\mathcal{N}(t_n|y(x_n, \\mathbb{w}), \\beta^{-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换为对数似然函数，得到：\n",
    "\n",
    "$$\\ln p(\\mathbb{t}|\\mathbb{x},\\mathbb{w}, \\beta) = -\\frac{\\beta}{2}\\sum_{n=1}^N\\{y(x_n, \\mathbb{w})-t_n)^2\\}+\\frac{N}{2}\\ln\\beta -\\frac{N}{2}\\ln(2\\pi)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过极大似然估计求解$\\mathbb{w}$，即最大化上述式子，等价于最小化上述式子相反数；同时，常数项与量纲项不影响结果，则上述式子可写为：\n",
    "\n",
    "$$\\min_{\\mathbb{w}} \\frac{1}{2}\\sum_{n=1}^N \\{y(x_n, \\mathbb{w})-t_n)^2\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过最小化上述式子求得$\\mathbb{w}_{ML}$后，再对似然函数关于$\\beta$求最小值，可以得到：\n",
    "\n",
    "$$\\frac{1}{\\beta_{ML}}=\\frac{1}{N}\\sum_{n=1}^N \\{y(x_n, \\mathbb{w}_{ML})-t_n)^2\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 我们注意到，$\\frac{1}{\\beta_{ML}}$是所有训练样本的均方误差，反映了参数$\\mathbb{w}_{ML}$关于样本的拟合程度。均方误差越大，说明参数未学习好，此时$\\beta_{ML}$越小；均方误差越小，此时$\\beta_{ML}$越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 确定参数点估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了上述参数估计结果后，我们带回原先的式子，可以得到：\n",
    "\n",
    "$$p(t|x, \\mathbb{w}_{ML},\\beta_{ML})=\\mathcal{N}(t|y(x, \\mathbb{w}_{ML}), \\beta_{ML}^{-1})$$\n",
    "\n",
    "但此时，我们给出的只是一个概率模型，给出了$t$的一个预测概率分布结果，并不是一个点估计结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更进一步的，从贝叶斯角度来分析，假设我们多项式函数前面的系数$\\mathbb{w}$的先验分布为多元高斯分布：\n",
    "\n",
    "$$p(\\mathbb{w}|\\alpha)=\\mathcal{N}(\\mathbb{w}|\\mathbb{0}, \\alpha^{-1}\\mathbb{I})=(\\frac{\\alpha}{2\\pi})^{(M+1)/2}\\exp{\\{-\\frac{\\alpha}{2}\\mathbb{w}^T\\mathbb{w}\\}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$\\alpha$控制了先验分布的precision，为超参数。根据贝叶斯理论，$\\mathbb{w}$的后验概率正相关于似然和先验概率的乘积：\n",
    "\n",
    "$$p(\\mathbb{w}|\\mathbb{x}, \\mathbb{t}, \\alpha, \\beta)\\propto p(\\mathbb{t}|\\mathbb{x}, \\mathbb{w}, \\beta)p(\\mathbb{w}|\\alpha)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上述公式，可以极大似然后验概率（Maximum Posterior）来求得适合于训练样本的$\\mathbb{w}$。对上述式子转化为负对数似然并带入先验概率分布可得，最大后验概率转化为最小以下函数：\n",
    "\n",
    "$$\\min_\\mathbb{w} \\frac{\\beta}{2}\\sum_{n=1}^N \\{y(x_n, \\mathbb{w})-t_n^2\\}+\\frac{\\alpha}{2}\\mathbb{w}^T\\mathbb{w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以观察到上述式子等价于$\\lambda=\\frac{\\alpha}{\\beta}$的带有正则化的error function。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在数据量充分的情况下，可以使用validation进行Model Selection，使用Test Data进行Model Evaluation;\n",
    "- 数据量有限时，使用cross-validation（leave-one-out是cross-validation的特例）。缺点是计算复杂度会非常高，如果有多个不同的超参数组合需要尝试，那么对于cross-validation来说，每一种参数都需要进行k次计算，复杂度过高。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于模型选择方面，有AIC和BIC这类量化指标：\n",
    "- AIC: Akaike Information Criterion\n",
    "- BIC: Bayesian Information Criterion\n",
    "\n",
    "这两类指标不考虑模型参数的不确定性，在实际中倾向于选择简单的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 维度灾难（The Curse of Dimensionality）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们有一个数据集，包含$D$维输入向量，以及每个点对应的类别；此时，当我们有一个新的点是，该如何判断其所属的类别呢？最简单的方法就是将$D$维空间进行切割，切为子空间，统计新样本点所在的子空间中各个类别训练样本的占比，新样本点的类别就可以视为占比最多的类别。\n",
    "\n",
    "上述方法有一个问题，当我们$D$非常大的情况下，我们所切分的cells也就变得很多，cells的数量呈指数增长；并且要求我们的数据量也足够大，以此保证每个cells中都有样本点存在，显然在实际中是难以操作的。\n",
    "\n",
    "回到多项式拟合问题中，当我们有$D$维的输入向量时，一般3阶多项式函数可以表达为：\n",
    "\n",
    "$$y(\\mathbb{x},\\mathbb{w})=w_0+\\sum_{i=1}^D w_i x_i + \\sum_{i=1}^D \\sum_{j=1}^D w_{ij}x_i x_j + \\sum_{i=1}^D \\sum_{j=1}^D \\sum_{k=1}^D w_{ijk} x_i x_j x_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随着$D$的增加，参数$w$数量会与$D^3$呈正比。在实际的建模中，我们往往需要使用高阶特征组合来capture数据的复杂性（参考FM，DeepFM，xDeepFM等）。但是随着多项式阶数$M$的增大，参数的规模会变为$D^M$，在实际中难以操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**球形例子**：假设在$D$维空间中有一个半径$r=1$的球，试求半径为$r=1$和半径为$r=1-\\epsilon$球体之间的体积占比。\n",
    "\n",
    "设在$D$维空间中，球形体积为$V_D(r) = K_D r^D$，其中$K_D$是关于$D$的一个常量。则所求部分的体积占比为$\\frac{V_D(1)-V_D(1-\\epsilon)}{V_D(1)}=1-(1-\\epsilon)^D$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将$\\epsilon$和$D$两个参数取不同值后画出，可以看到，即使当$\\epsilon$很小时，只要维度$D$足够大，高维球体中绝大部分体积都集中在一个贴近球面的超薄曲面上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ch1/curse_of_dimension.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "维度灾难会带来的问题：\n",
    "- 1.随着维度上升，在高维空间中样本会非常稀疏，且多数样本会集中在靠近高维空间角落位置，而中心点附近样本极少；\n",
    "- 2.由于样本稀疏性，模型很容易学习到training data中的异常特性，使得模型出现过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尽管curse of dimensionality会造成一些问题，但是并不妨碍我们使用有效的技术解决高维空间问题。\n",
    "\n",
    "- 1.实际中的数据维度往往可以被精简到少数几个有效的维度，即降低了有效维度空间（例如挑选对目标重要的特征）；\n",
    "- 2.降维，例如PCA\n",
    "- 3.显示数据往往表现出良好的局部平滑性质，因此在大多数情况下，特征的局部变化会造成目标变量的局部变化，因此我们可以利用局部类似线性插值的技术来对新的输入变量进行预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Decision Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定输入向量$\\mathbb{x}$的情况下，利用贝叶斯理论，样本类别的后验概率为：\n",
    "\n",
    "$$p(\\mathcal{C}_k|x)=\\frac{p(x|\\mathcal{C}_k) p(\\mathcal{C}_k)}{p(x)}$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $p(\\mathcal{C}_k)$为先验概率\n",
    "- $p(\\mathcal{C}_k|x)$为后验概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 最小化误分类率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们的决策目标是尽可能减少误分类率。因此我们首先需要一个规则来将输入向量$x$映射到一个类别上，这样的规则最终可以将整个输入空间划分为多个决策区域（decision regions），记为$\\mathcal{R}_k$，属于$\\mathcal{R}_k$中的所有样本点将被标记为$\\mathcal{C}_k$。（决策区域不一定是连续的，但必须包括多个不相交的区域）。\n",
    "\n",
    "不同决策区域中的边界即为决策边界（decision boundaries）或决策平面（decision surfaces）。\n",
    "\n",
    "假设两个类别$\\mathcal{C}_1$和$\\mathcal{C}_2$，我们误分类率可以记为：\n",
    "\n",
    "$$p(mistake)=p(x\\in \\mathcal{R}_1, \\mathcal{C}_2) + p(x \\in \\mathcal{R}_2, \\mathcal{C}_1)\\\\ =\\int_{\\mathcal{R}_1} p(x, \\mathcal{C}_2)dx + \\int_{\\mathcal{R}_2} p(x, \\mathcal{C}_1)dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了使上述误分类率最小，我们应该把$x$的类别分配给使得积分较小的那一类；因此，对于给定的$x$，如果$p(x,\\mathcal{C}_1) \\gt p(x, \\mathcal{C}_2)$，那么我们需要将$x$的类别判别为$\\mathcal{C}_1$。\n",
    "\n",
    "由于$p(x, \\mathcal{C}_k) = p(\\mathcal{C}_k|x)p(x)$，且$p(x)$是固定项，我们可以说，最小化误分类率就等价于将$x$分配给其后验概率最大的那一类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图例：\n",
    "\n",
    "下图展示了给定两个类别下，样本点$x$与类别的联合概率分布。假设我们的输入向量只有一个维度，那么$p(x,\\mathcal{C}_1)$就是在真实样本中类别属于$\\mathcal{C}_1$的样本特征$x$的概率分布，$p(x,\\mathcal{C}_2)$同理。\n",
    "\n",
    "我们可以发现两个分布会有交集，意味着当$x$取值相同时，其有可能属于不同的类别。假设我们的决策边界为$x=\\hat{x}$，此时误分类率就是图中红、绿、蓝区域的面积。\n",
    "\n",
    "我们发现绿色和蓝色区域是一个常量，无论如何移动决策边界，其面积固定。因此当$\\hat{x}=x_0$时，此时红色区域消失，我们得到了最小误分类率。此时，我们的决策结果是，将$x$的类别分配给联合概率分布$p(x, \\mathcal{C}_k)$大的类别，等价于最大化后验概率$p(\\mathcal{C}_k|x)$。\n",
    "\n",
    "![](ch1/decision-theory.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
