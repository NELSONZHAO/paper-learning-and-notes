{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 概念\n",
    "\n",
    "学习与智能理论的基础思想：从交互经历中学习（Learning from interaction）\n",
    "\n",
    "强化学习是一种通过**计算统计方法（Computational approach）**，以**目标为导向（goal-directed）**从**与环境的交互**中学习的方式。\n",
    "\n",
    "强化学习也是一种通过学习在不同**情境环境（situation）**中采取不同**行动（actions）**以此来最大化**数值激励信号（numerical reward signal）**的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 特点\n",
    "\n",
    "- Trial-and-error search:从在环境中试错与交互进行学习\n",
    "- Delayed reward:\n",
    "    - 当前action不仅影响当前的reward，还可能影响后续的环境以及rewards\n",
    "    - 很多情境下，rewards不能及时反馈，需要交互多次才会产生reward反馈\n",
    "- goal-seeking(goal-directed):目标导向，累积激励最大化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 与其它学习方法比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|||||\n",
    "|-|-|-|-|\n",
    "|**学习方法**|监督学习|无监督学习|强化学习|\n",
    "|学习输入|环境的描述（特征）+正确的标签（目标）|无标签的数据|经历（experience）+奖励（reward）|\n",
    "|学习目标|预测与泛化未出现在学习样本中的情景|无标签数据背后的隐藏结构信息（hidden structure）|在未知领域探索收益最大化的策略|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 强化学习的例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1.象棋高手在比赛中下一步棋。在思考当前一步下什么棋子时，需要思考对手的应对措施以及自己未来该如何应对，同时也需要一些直观判断；\n",
    "- 2.自适应控制器实时调整炼油厂操作的参数。通过与环境状态的交互来确定当前参数；\n",
    "- 3.刚出生的小动物学习走路；\n",
    "\n",
    "\n",
    "总结上述例子中的特点：\n",
    "- 有实体与环境的交互\n",
    "- 在不确定性环境下完成自己的目标\n",
    "- 当前决策会影响后续的一系列环境和决策（这也是与监督学习不同的一点，监督学习仅做一次推断决策，且各个样本之间独立；而RL的决策具有序列依赖关系）\n",
    "- 由于决策还会影响后续的情景和决策，因此当下决策需要看的更加长远\n",
    "- agent必须能够时刻关注环境变化并及时作出决策\n",
    "- agent可以通过与环境的交互提高未来的performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 强化学习的要素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Policy 策略\n",
    "\n",
    "策略本身是一个映射，它将当前时刻下的环境映射为特定的action（在心理学中类似stimulus-response规则）。策略是强化学习的核心，它既可以是简单的函数，或者look-up表格，也可以是复杂的搜索计算。在随机性上，policy既可以是deterministic，也可以是stochastic的。\n",
    "\n",
    "### (2) Reward Signal 激励\n",
    "\n",
    "Reward是强化学习中对目标（goal）的一种量化定义。在每一次agent与环境交互中会获得一个reward，agent的目的就是最大化长期的累积reward。Reward signal定义了agent采取行动的好和坏，例如是正确的行动就给予正值，错误的行动给予负值（心理学中reward可以类比为人类开心的经历和痛苦的经历）。\n",
    "\n",
    "Reward是agent用来更新policy的重要参考，如果某一个policy产生的action得到了低reward，agent就会去更新和调整policy来使得reward增加。从数学上来看，reward是关于state和action的随机函数。\n",
    "\n",
    "### (3) Value Function 值函数\n",
    "\n",
    "Reward反应的是当前时刻下的反馈，value function定义了在长期(long run)中什么是好的。\n",
    "\n",
    "The state of value是指从当前state开始起，agent在未来能够获得的期望累积奖励值。\n",
    "\n",
    "类比到我们的生活中，reward反应的是当下的快乐或痛苦，而value反映了更长远的幸福或者折磨等。\n",
    "\n",
    "> Reward和Value比较：\n",
    "> - Reward是Value的基础，有了Reward，才可以计算出Value;\n",
    "> - 在决策过程中，我们参考的是Value而不是Reward，因为Value能够带来长期收益最大化;\n",
    "> - Reward可以直接由环境给出，然而Value必须靠一系列的观察和统计来进行预测和估计。\n",
    "\n",
    "### (4) Model 环境模型\n",
    "\n",
    "环境模型主要用来模拟agent所处的环境情况，它可以推断出未来的环境变化以及agent采取不同action时带来的不同环境变化等。环境模型主要用来做Planning，即agent可以在行动之前，预估本次行动带来的未来环境变化。\n",
    "\n",
    "在强化学习中分为两类方法：\n",
    "- Model-based: 使用环境模型来做planning\n",
    "- Model-free: 试错学习(trial-and-error/test-and-learn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
