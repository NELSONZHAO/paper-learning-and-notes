{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 多臂老虎机问题（K-armed Bandit Problem）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 问题简介\n",
    "\n",
    "在赌场中，有k个老虎机。在每个时刻下，Agent可以选择一个老虎机进行操作，相应地会得到一个reward，agent的目标就是在有限次操作（或时间步下）获得最大期望收益。\n",
    "\n",
    "在k-armed老虎机问题中，每个时间步可以有k个选择，我们定义在时间步$t$下选择的action为$A_t$，相应的奖励为$R_t$。定义**任意一个动作$a$的value为$q_*(a)=\\mathbb{E}[R_t|A_t=a]$**。\n",
    "\n",
    "当我们知道每个action的value时，我们可以很容易解决k-armed问题，即我们总是选择那个具有最高value的action。但实际中，每个老虎机的reward符合某一个分布，我们并不知道每个action具体能得到多少回报，我们需要对回报进行估计，定义在时刻$t$下对动作$a$的估计value为$Q_t(a)$，我们希望$Q_t(a)$越接近$q_*(a)$越好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Exploitation vs. Exploration\n",
    "\n",
    "如果在每个时间步下，我们总选择估计value最大的action，我们称这个动作为**Greedy Action**，这时我们称为**Exploiting**；当我们选择其他非贪心动作时，称为**Exploring**。\n",
    "\n",
    "Exploitation和Exploration对比：\n",
    "- Exploiting能够在当前时间步获得最大期望收益，但Exploring可能会在长期产生最大收益；\n",
    "- Exploiting使用的是当前已知的最大收益action，但实际在其他nongreedy的动作中存在未来收益更大的动作，只有通过Exploring，才可能对这些nongreedy动作value进行更新，从而得知更优的动作；\n",
    "- 如果未来时间步很长，往往都需要去Exploring，更可能带来长期的最大收益。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 动作值方法（Action-value Methods）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 动作值估计\n",
    "Action-value Methods: 通过估计动作的值并且将估计结果用于动作决策过程的方法。\n",
    "\n",
    "我们定义一个action的true value是**当这个action被选择时，其获得的所有reward的期望**，因此我们可以这样计算：\n",
    "\n",
    "$$Q_t(a)=\\frac{\\sum_{i=1}^{t-1}R_i\\cdot \\mathbb{1}_{A_i=a}}{\\sum_{i=1}^{t-1}\\mathbb{1}_{A_i=a}}$$\n",
    "\n",
    "分子是当前$t$时刻之前所有选择动作$a$时产生的回报之和，分母是过去选择动作$a$的次数。如果分母为0，就设置为默认值。随着分母趋近于无穷大，$Q_t(a)$会趋近于$q_*(a)$。这种方法被称为**sample-average**方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 动作选择\n",
    "\n",
    "### 2.2.1 Greedy Methods\n",
    "当我们对Action value估计完成后，需要应用到动作选择中去。最简单的是采用贪心策略：\n",
    "\n",
    "$$A_t=\\arg\\max_a Q_t(a)$$\n",
    "\n",
    "这种方式称为**greedy**方法，它完全是基于过去的知识，并且不需要去评估其它action是否会在未来的值更大。\n",
    "\n",
    "### 2.2.2 $\\epsilon$-Greedy Methods\n",
    "\n",
    "贪心算法的不足之处在于它永远不会去探索潜在更好的action，属于保守型。另一种方法是以$\\epsilon$的概率在所有action中随机选择，以$1-\\epsilon$的概率选择值最大的动作：\n",
    "\n",
    "$$A_t=\\begin{cases}\\arg\\max_a Q_t(a)\\ with\\ 1-\\epsilon+\\epsilon/nA \\\\ a\\ with\\ \\frac{\\epsilon}{nA}\\end{cases}$$\n",
    "\n",
    "这种方法的优点是当步数趋近于无穷大时，每个action都会被采样无数次，保证了所有action的$Q_t(a)$收敛于$q_*(a)$，也就意味着所有最优action的收敛概率大于$1-\\epsilon$。\n",
    "\n",
    "> 在$\\epsilon-greedy$中，选择greedy action的概率为$1-\\epsilon+\\epsilon/nA$，其中$nA$为所有动作数量；选择non-greedy action的概率为$\\epsilon/nA$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 动作方法对比\n",
    "\n",
    "从通用的情景对比来看：\n",
    "- Greedy Method能够很快在Experience的初始阶段达到局部最优，但随后无法跳出当前局部最优，因而难以达到全局最优；类似一个人总是停留在“舒适区”一样，不去尝试一下永远不会知道还有更好的解；\n",
    "- $\\epsilon-greedy$在开始时的期望reward比较小，且增加缓慢，但随着experience的不断增加，最终能够达到近似全局最优的点；且在Experience的初始阶段，可以将$\\epsilon$设置大一点，让agent去exploring，随后对$\\epsilon$进行衰减，即让agent去exploiting。\n",
    "\n",
    "\n",
    "从不同情况来对比：\n",
    "- 当Bandit的reward方差很大时，意味着reward值较分散，因此需要$\\epsilon-greedy$这种更加探索性的方式；\n",
    "- 当Bandit的reward分布方差为0时，此时Greedy方法能够达到最优；\n",
    "- 当Bandit的reward分布随着时间发生变化时，使用Greedy很容易陷入局部最优；因为有些action的reward很有可能在未来变得更好，这也是强化学习中经常碰到的情景，因此$\\epsilon-greedy$是更好的方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue' >Exercise 2.2<br>\n",
    "    \n",
    "Bandit example Consider a k-armed bandit problem with k = 4 actions, denoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm using \"-greedy action selection, sample-average action-value estimates, and initial estimates of Q1(a) = 0, for all a. Suppose the initial sequence of actions and rewards is A1 = 1, R1 =1,A2 =2,R2 =1,A3 =2,R3 =2,A4 =2,R4 =2,A5 =3,R5 =0. Onsome of these time steps the \" case may have occurred, causing an action to be selected at random. On which time steps did this definitely occur? On which time steps could this possibly have occurred?<br>\n",
    "\n",
    "Answer: Q_1(1) = 1，此时选择Action=1是Greedy，但下一步的A2=2，因此在t1时刻$\\epsilon$产生了作用。同理，可得以下结果：\n",
    "\n",
    "- T1时刻，$\\epsilon$一定产生作用；\n",
    "- T2时刻，$\\epsilon$可能产生作用；\n",
    "- T3时刻，$\\epsilon$可能产生作用；\n",
    "- T4时刻，$\\epsilon$一定产生作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Exercise 2.3<br>\n",
    "\n",
    "In the comparison shown in Figure 2.2, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be? Express your answer quantitatively.<br>\n",
    "\n",
    "Answer: $\\epsilon-greedy$方法且系数为0.1时，能够在长期获得最大收益。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 计算方法实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们定义：\n",
    "\n",
    "- $R_i$是agent在第$i$次选择当前这个action时，获得的奖励；\n",
    "- $Q_n$代表这个action被选择$n-1$后的估计action-value。\n",
    "\n",
    "则有：$Q_n=\\frac{R_1+R_2+\\cdots +R_{n-1}}{n-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在具体实现过程中，上述公式意味着我们需要维护每个action的累积Reward和被选择的次数，在内存与计算效率上都是非常低的。\n",
    "\n",
    "公式化简：\n",
    "\n",
    "$$Q_{n+1}=\\frac{1}{n}\\sum_{i=1}^n R_i\\\\=\\frac{1}{n}(R_n+\\sum_{i=1}^{n-1}R_i)\\\\=\\frac{1}{n}(R_n+(n-1)\\frac{1}{n-1}\\sum_{i=1}^{n-1}R_i)\\\\=\\frac{1}{n}(R_n+(n-1)Q_n)\\\\=\\frac{1}{n}(R_n+nQ_n-Q_n)\\\\=Q_n+\\frac{1}{n}[R_n-Q_n]$$\n",
    "\n",
    "经过简化以后，只需要维护每个action的$Q_n$和$n$即可。\n",
    "\n",
    "更一般地，这个公式可以表达为：\n",
    "\n",
    "$$NewEsitimate \\leftarrow OldEstimate + StepWise\\ [Target-OldEstimate]$$\n",
    "\n",
    "其中$[Target-OldEstimate]$是估计中的error，每次Q的更新都往Target走的更近一步，Target定义方向，$\\frac{1}{n}$定义了步伐，更一般地，可以定义为$\\alpha$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/bandits-q-estimation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
