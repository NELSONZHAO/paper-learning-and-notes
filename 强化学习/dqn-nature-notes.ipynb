{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 摘要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "传统强化学习中，Agent面临着一些困难：\n",
    "- 从高维感知环境的输入中提取有效的环境表达（Representations of the environment）\n",
    "- 使用这些表达去总结概括过去经验，从而应对新的环境\n",
    "\n",
    "过去强化学习的应用受限于某些领域，例如可以人工构造特征的领域或者某些具有完全可观察的、低维的状态空间领域下。\n",
    "\n",
    "提出利用DNN去训练新的智能体Agent的方式，称为**（deep Q-network）**，能够使用端到端的强化学习方式直接从高维输入习得最优策略。\n",
    "\n",
    "DQN在Atari经典的2600个游戏上经过测试，取得了超越过去所有算法性能的效果。通过接收像素与游戏分数作为输入，可以取得非常好的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 背景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们假设Agent面临的任务是通过观察(observations)、动作(actions)以及奖励(rewards)的序列来与环境交互，即任务可抽象为以下序列：\n",
    "\n",
    "$$(s_0,a_0,r_1,s_1,a_1,\\dots,r_T,s_T,a_T)$$\n",
    "\n",
    "Agent目标是以某一种方式选择actions来最大化累积未来奖励(cumulative future reward)。本文中，作者使用深度卷积神经网络来近似最优的action-value function：\n",
    "\n",
    "$$Q^*(s,a)=\\max_\\pi\\mathbb{E}[r_t+\\gamma r_{t+1} + \\gamma^2 r_{t+2}+\\dots +|s_t=s, a_t=a, \\pi]$$\n",
    "\n",
    "当使用非线性激活函数近似来表达action-value(Q) function时，强化学习的表现是不稳定且容易发散的，原因如下：\n",
    "\n",
    "- 观察序列之间的相关性（不够独立，因此只要Q发生一点变化，后续的Policy会因引起蝴蝶效应）\n",
    "- action-value(也称Q值)和目标值$r+\\gamma\\max_{a'}Q(s',a')$之间的相关性\n",
    "\n",
    "现存的方法例如neural fitted Q-iteration可以解决这个问题，但不够高效。因此作者提出两种新的关键idea来解决上述问题：\n",
    "\n",
    "- **经验回放(Experience Replay)**\n",
    "- **Q值的周期性迭代更新(Periodically Updated)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 相关工作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
