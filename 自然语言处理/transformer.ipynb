{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 摘要\n",
    "\n",
    "过去主流序列传导模型(Sequence Transduction Models)都是基于循环神经网络RNN或者卷积神经网络CNN作为特征提取器，表现比较好的模型则需要在Encoder-Decoder中加入Attention机制。\n",
    "\n",
    "本文提出一个简单的网络结构，称为**Transformer**，这个结构完全**不需要依赖RNN或者CNN**，而仅仅是建立在**Attention Mechanisms**之上，因此也使得模型更易并行，加快训练速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 简介与背景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 现有不足\n",
    "\n",
    "- RNN模型天然的串行序列结构限制计算并行化\n",
    "- Attention机制往往需要和RNN模型进行组合\n",
    "- Extended Neural GPU, ByteNet, ConvS2S虽然使用了CNN并行化，但是关联输入与输出之间位置关系的计算操作数会随着距离增大而增大（ConvS2S是线性增加，ByteNet是对数增加），因此不利于模型学习远距离关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 背景\n",
    "\n",
    "- Transformer将计算位置之间关系的操作数降低为常数级别\n",
    "- 使用了Multi-Head Attention\n",
    "- 使用Self-Attention(Intra-Attention)对序列进行表达"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 模型结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Encoder\n",
    "\n",
    "- 由6层相同的层堆叠\n",
    "- 每一层包含两个sub-layer:\n",
    "    - multi-head self-attention menchanism\n",
    "    - position-wise fully connected feed-forward network\n",
    "- 在每个sub-layer上使用了residual connection（残差连接），后面接了一个Layer Norm结构($LayerNorm(x+Sublayer(x))$)\n",
    "- 为了方便这些residual connections，所有的sub-layer包括embedding层的输出维度都是512维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Decoder\n",
    "\n",
    "- 由6层相同的层堆叠\n",
    "- 每一层包含三个sub-layer:\n",
    "    - multi-head self-attention menchanism\n",
    "    - fully connected feed-forward network\n",
    "    - masked-multi-head self-attention(保证输出序列中第i位置的元素时，只对前i-1个元素做self-attention)\n",
    "- 在每个sub-layer上使用了residual connection（残差连接），后面接了一个Layer Norm结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Attention结构\n",
    "\n",
    "将给定的一个query和一个key-value pairs集合映射为一个加权输出。\n",
    "\n",
    "> 类比Seq2Seq中的Attention机制。\n",
    "> - Decoder端的当前t时刻隐藏层向量$s_t$为query\n",
    "> - Encoder端的所有timestep下的隐藏层向量构成了key集合$(h_1, h_2, \\cdots, h_t)$，value和key在Seq2Seq中是一样的\n",
    "> - Attention是关于query和key的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Scaled Dot-Product Attention\n",
    "\n",
    "输入为一个query和一个集合keys，这两个都是$d_k$维向量，以及values为$d_v$维向量。将query和每个keys进行内积运算，并对结果除以$\\sqrt{d_k}$缩放后进入Softmax Layer，得到权重。\n",
    "\n",
    "![](pictures/scaled-dot-product-attention.png)\n",
    "\n",
    "在实际计算中，会对多个queries打包为矩阵$Q$后进行并行计算。keys和values也被打包为矩阵$K$和矩阵$V$：\n",
    "\n",
    "$$\\mathcal{Attention}(Q, K, V)=\\mathcal{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "> 常用的Attention计算方法有additive和multiplicative两种方式。前者是将q和k拼接起来训练一层神经网络，后者是计算q和k的内积。这两种的理论复杂度上是同量级的，但实际中dot-product的方式在速度上更快，且space-efficient。\n",
    "\n",
    "> 作者提到当$d_k$比较小的时候，additive和multiplicative两种attention表现差不多；但当$d_k$变大时，如果都不进行scale，addictive会表现更优。作者猜测是由于$d_k$导致向量内积时出现量纲过大，将内积结果输入softmax后落在了梯度饱和区域，导致没有学好。因此作者才加了$d_k$进行scale。（个人认为有点像做白化或者归一化，或者类似BN的操作，让输入落在非饱和区能够比较好的学习）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Multi-Head Attention\n",
    "\n",
    "作者发现使用多个Attention会使结果更好。因此提出了Multi-Head Attention结构，如下图所示。先将Q、K、V经过不同的h个线性投影（就是一层线性神经网络不加激活函数）后，再进行Scaled Dot-Product Attention的计算。\n",
    "\n",
    "![](pictures/multi-head-attention.png)\n",
    "\n",
    "Multi-head attention的点在于能够让模型从不同的表征子空间去共同学习不同位置的表达信息。其实有点类似于在CNN中的多个filter去学习图片不同的表达信息，multi-head attention也可以学习到不同的语义信息。\n",
    "\n",
    "$$\\mathcal{MultiHead}(Q,K,V)=\\mathcal{Concat}(head_1, head_2, \\cdots, head_h)W^O$$\n",
    "\n",
    "$$head_i = \\mathcal{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "\n",
    "其中$W$指线性映射层的参数：\n",
    "\n",
    "- $W_i^Q$是指$Q$的第i个线性映射参数，$W_i^Q\\in \\mathbb{R}^{d_{model}\\times d_k}$\n",
    "- $W_i^K$是指$K$的第i个线性映射参数，$W_i^K\\in \\mathbb{R}^{d_{model}\\times d_k}$\n",
    "- $W_i^V$是指$V$的第i个线性映射参数，$W_i^V\\in \\mathbb{R}^{d_{model}\\times d_v}$\n",
    "- $W^O$是指$Q$的第i个线性映射参数，$W^O\\in \\mathbb{R}^{hd_v\\times d_{model}}$\n",
    "\n",
    "在本文中，作者使用了$h=8$个并行的attention heads。对每个head，使用了$d_k=d_v=d_{model}/h=64$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Attention在Transformer中应用\n",
    "\n",
    "- **Encoder-Decoder Attention**。类似在Seq2Seq中，假设当前预测t时刻的词，queries来自前一层Decoder的输出，keys和values来自于input。这样使得Decoder端可以对Input序列的所有词都做Attention\n",
    "- **Encoder Self-Attention**。在Encoder中，Q,K,V都来自前一层Encoder的输出，使得Encoder中当前每个位置都可以处理来自上一层Encoder的所有输出。\n",
    "- **Decoder Self-Attention**。在Decoder中，由于预测时候是augo-regressive的，因此必须对t时刻右侧位置的词做mask，只对左侧部分的词做self-attention。在实际建模中，对softmax函数mask部分的输入赋为$-\\infty$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Position-wise Feed-Forward Networks\n",
    "\n",
    "在每个Encoder层中，除了Attention Sub-Layer，后面还接了一层前向全连接层：\n",
    "\n",
    "$$FFN(x)=\\max(0, xW_1+b_1)W_2+b_2$$\n",
    "\n",
    "注意到实际上是一个两层网络，第一层采用了ReLU激活函数，后一层是线性函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Embeddings and Softmax\n",
    "\n",
    "文中使用词向量Embeddings会跟着Model进行训练，将词输入token转化为$d_{model}$维向量。同样在预测时候，采用一般的线性变换加Softmax得到预测的token。文中所有的词向量是共享的。在Embedding层，每个权重都乘以了$\\sqrt{d_{model}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Positional Encoding\n",
    "\n",
    "由于Transformer完全没有使用CNN和RNN，因此需要额外的信息来告诉模型输入token的相对位置(两个token之前的相对距离)和绝对位置(当前token在句子中的位置)信息。作者在Encoder输入端加入了positional encodings，这个位置向量和Embedding具有相同维度$d_{model}$，然后这两个词向量相加后再输入模型。\n",
    "\n",
    "有很多种Positional Encodings的方法，包括固定的或者和可学习的。作者文中使用了不同频率（frequency）的sin和cos函数：\n",
    "\n",
    "$$PE(pos, 2i)=\\sin(pos/10000^{2i\\ /\\ d_{model}})$$\n",
    "\n",
    "$$PE(pos, 2i+1)=\\cos(pos/10000^{2i\\ /\\ d_{model}})$$\n",
    "\n",
    "其中$pos$代表位置，$i$代表维度。可以看到对于PE的每个维度$i$都是一个正弦函数。\n",
    "\n",
    "我们固定PE的某一个维度$i=d'$。则对于输入句子中的所有词，其有各自对应的$pos=(p_1,p_2, \\cdots, p_n)$。将$pos$作为横轴，每个词位置$p_i$对应在第$d'$维度的值作为纵轴。则$PE(pos, 2i)$可以写作$PE(pos, 2d')=\\sin((\\frac{1}{10000^{2d'/d_{model}}})pos)$，这个函数的周期（或波长）为$2\\pi\\cdot 10000^{2d'/d_{model}}$。\n",
    "\n",
    "作者尝试了可学习的PE，发现和Fixed结果差不多。最后选择sin和cos有两点好处：\n",
    "- 不受限于Training样本句子长度限制，可以让模型在Inference阶段推断比Training阶段看到的更长的句子\n",
    "- 对于任意固定的偏移量$k$，$PE_{pos+k}$都可以被表示成$PE_{pos}$的线性函数\n",
    "    - $\\sin(a+b)=\\sin(a)\\cos(b)+\\sin(b)\\cos(a)$\n",
    "    - $\\cos(a+b)=\\cos(a)\\cos(b)-\\sin(a)\\sin(b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 以$\\sin$函数为例，$y=A\\sin(Bx+C)+D$，其中\n",
    "> - A代表振幅(Amplitude)\n",
    "> - 周期(Period)为$\\frac{2\\pi}{B}$，频率(Frequency)为$1/Period=\\frac{B}{2\\pi}$\n",
    "> - 相移(Phase Shift)为$-\\frac{C}{B}$\n",
    "> - 垂直位移(Vertical Shift)为$D$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Transformer中的Encoder和Decoder端都使用了self-attention，转换到数学上就是讲原来的变长序列$(x_1, x_2, \\cdots, x_n)$映射到另一个等长的序列$(z_1, z_2, \\cdots, z_n)$，其中$x_i,z_i\\in \\mathbb{R}^d$。在Seq2Seq中，$x$可以对应原始的Word Embedding，$z$可以对应LSTM输出的hidden state。\n",
    "\n",
    "作者提出Transformer需要Self-Attention结构的考虑方面在于：\n",
    "- 每一层的所有计算复杂度\n",
    "- 所有可以并行的计算量（用最小序列操作数衡量）\n",
    "- 学习路径中长依赖关系的能力。影响模型学习长依赖能力的关键在于前向和反向信号之间的路径长度，当input和output中任意位置单词之间的期望路径越短，越容易学习这种依赖关系\n",
    "\n",
    "下图比较了不同网络结构中的最大长度，其中$n$为序列长度，$d$是Embedding维度：\n",
    "\n",
    "||||\n",
    "|-|-|-|-|\n",
    "|Layer Type|Complexity Per Layer|Sequential Operations|Maximum Path Length|\n",
    "|Self-Attention|$O(n^2\\cdot d)$| $O(1)$ | $O(1)$ |\n",
    "|Recurrent|$O(n\\cdot d^2)$|$O(n)$|$O(n)$|\n",
    "|Convolutional|$O(k\\cdot n\\cdot d^2)$|$O(1)$|$O(\\log_k(n))$|\n",
    "|Self-Attention(restricted)|$O(r\\cdot n \\cdot d)$|$O(1)$|$O(n/r)$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Residual Dropout\n",
    "\n",
    "- 对Sub-layer的输出进行Dropout，在LayerNorm之前进行\n",
    "- 对Embeddings和Positional Encodings求和结果进行Dropout\n",
    "- 使用$P_{drop}=0.1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Label Smoothing\n",
    "\n",
    "类似GAN中一样，这里采用了Label Smoothing，参数为$\\epsilon_{ls}=0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
