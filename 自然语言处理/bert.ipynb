{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 摘要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT全称Bidirectional Encoder Representations from Transformers. BERT模型旨在基于无标签文本的上下文来预训练词的深度双向表征。预训练好的BERT模型可以简单地在下游接一层输出层，从而在不同任务上达到state-of-art的预测效果，例如QA、语言推断。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在NLP任务中，预训练词向量被证明能够起到很好的作用。在实际应用中，有两种将预训练向量用在下游任务的方式：\n",
    "\n",
    "- 1.**feature-based**。例如ELMo使用task-specific的模型结构将预训练向量作为额外的特征；\n",
    "- 2.**fine-tuning**。例如GPT引入极少的task-specific参数并在下游任务训练过程中对词向量做fine-tuning。\n",
    "\n",
    "上面这两种方法在预训练过程中都使用了相同的目标函数，即使用单向语言模型(unidirectional Language)去学习通用的语言表征。例如在GPT中，作者使用了从左到右的单向结构，使得每个token在做self-attention的时候都只能看到之前的token（参考Transformer的Decoder），这种单向模型叫做masked language model(MLM)；这样的限制使得token无法获取上下文完整信息，对于学习的词向量有极大的影响，尤其在QA这种强依赖上下文信息的任务中。\n",
    "\n",
    "在这篇论文中，作者提出了BERT，通过两个子任务缓解了之前提到的MLM问题：\n",
    "\n",
    "- 1.随机对一个句子中的某个token进行遮盖，利用上下文信息来预测这个token；\n",
    "- 2.\"Next Sentence Prediction\"任务，来联合预训练text-pair（相邻句子上下文信息）的表征。\n",
    "\n",
    "论文的主要贡献有是三个方面：\n",
    "\n",
    "- 1.证明了Bidirectional pre-training for language representions的重要性；\n",
    "- 2.pre-trained representations可以大量减少下游复杂的模型架构和参数；\n",
    "- 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
